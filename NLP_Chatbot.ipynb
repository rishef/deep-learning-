{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "OVKJzYBwejHK",
        "outputId": "109132f3-aaf4-45d9-92c0-42be76265848",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2==2.8.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMkJOCjyMQ8K",
        "outputId": "535ec877-22b7-42ca-e080-b9374455c3a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2==2.8.1 in /usr/local/lib/python3.10/dist-packages (2.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import PyPDF2\n",
        "import os"
      ],
      "metadata": {
        "id": "NQ7Wqy-RWgu_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#%% Run the code below [ONLY ONCE] when running the code for the first time ever\n",
        "# Downloading tokenizer file for model using python as file is huge\n",
        "\n",
        "import requests\n",
        "import json\n",
        "name = 'tokenizer.json'\n",
        "url = f'https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/raw/main/{name}'\n",
        "resp = requests.get(url, verify=False)\n",
        "dict_ = resp.json()\n",
        "json_object = json.dumps(dict_, indent = 4)\n",
        "with open(f'D:\\\\Projects\\\\bhoonidhi\\\\bert-large-uncased-whole-word-masking-finetuned-squad\\\\{name}', \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "url = 'https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/raw/main/vocab.txt'\n",
        "resp = requests.get(url,verify=False)\n",
        "dict_ = resp.text\n",
        "with open(f'D:\\\\Projects\\\\bhoonidhi\\\\bert-large-uncased-whole-word-masking-finetuned-squad\\\\vocab.txt', \"w\", encoding=\"utf-8\") as outfile:\n",
        "    outfile.write(dict_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MbzeA8JWgyB",
        "outputId": "6b87d6aa-febb-44dd-b700-cf80686ac557"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Defining function for pdf search\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Vwa1b_TKWg09"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# !pip install pdfplumber\n",
        "# import pdfplumber\n",
        "\n",
        "# pdf_path = \"/content/drive/MyDrive/Union of India vs Ved Prakash Mithal and Sons- DELHI HC JUDGEMENT (1).pdf\"\n",
        "\n",
        "# with pdfplumber.open(pdf_path) as pdf:\n",
        "#     for page in pdf.pages:\n",
        "#         text = page.extract_text()\n",
        "#         print(text)"
      ],
      "metadata": {
        "id": "NtvayEjkdkO2",
        "outputId": "8281d26b-b47e-4004-e611-d66516eaca7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.29.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (42.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PDFSyntaxError",
          "evalue": "No /Root object! - Is this really a PDF?",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPDFSyntaxError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c017f50c2ba3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpdf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Union of India vs Ved Prakash Mithal and Sons- DELHI HC JUDGEMENT (1).pdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mpdfplumber\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pdfplumber/pdf.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, path_or_fp, pages, laparams, password, strict_metadata, repair, gs_path)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             return cls(\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pdfplumber/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stream, stream_is_external, path, pages, laparams, password, strict_metadata)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPDFParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsrcmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFResourceManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pdfminer/pdfdocument.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parser, password, caching, fallback)\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPDFSyntaxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No /Root object! - Is this really a PDF?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mLITERAL_CATALOG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTRICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPDFSyntaxError\u001b[0m: No /Root object! - Is this really a PDF?"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Open the PDF file\n",
        "pdf_path = \"/content/Union of India vs Ved Prakash Mithal and Sons- DELHI HC JUDGEMENT.pdf\"\n",
        "pdfFileObject = open(pdf_path, 'rb')\n",
        "pdfReader = PyPDF2.PdfReader(pdfFileObject)\n",
        "\n",
        "# Import Model and tokenizer\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "# Read PDF for the context\n",
        "count = pdfReader.numPages\n",
        "text = ''\n",
        "\n",
        "# Now you can proceed with using pdfReader and other variables\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G34tzJ6WubI",
        "outputId": "06af856a-af5b-429e-9bf8-3a3223c7b714"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T16VUGb7Lbjg",
        "outputId": "fa228feb-4116-4665-83ab-8d3fcf522ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching Page 1/7\n",
            "Question : What was M/s. Prakash Atlanta's case?\n",
            " Answer  : Nationalhighways authority of india\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import PyPDF2\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "def find_factors(x):\n",
        "    x = x - 1\n",
        "    factor = x\n",
        "    for i in range(1, x + 1):\n",
        "        if x % i == 0:\n",
        "            if not i > 512:\n",
        "                factor = i\n",
        "    if factor == 1:\n",
        "        factor, x = find_factors(x)\n",
        "    return factor, x\n",
        "\n",
        "def findAnswerPDF(question, pdf_path, model_path=None):\n",
        "    if not os.path.exists(pdf_path):\n",
        "        raise Exception('PDF file does not exist!')\n",
        "    if model_path:\n",
        "        if not os.path.exists(os.path.join(model_path, 'bert-large-uncased-whole-word-masking-finetuned-squad')):\n",
        "            raise Exception('Model files do not exist!')\n",
        "\n",
        "    answer = ''\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        count = len(pdf_reader.pages)\n",
        "\n",
        "        if model_path:\n",
        "            os.chdir(model_path)\n",
        "\n",
        "        for num in range(count):\n",
        "            if len(answer) > 0:\n",
        "                break\n",
        "\n",
        "            print(f'Searching Page {num + 1}/{count}')\n",
        "            page = pdf_reader.getPage(num)\n",
        "            temp = page.extractText()\n",
        "\n",
        "            if len(temp.replace('.', '').replace('\\n', '')) > 50:\n",
        "                # Clean the text from PDF\n",
        "                text = temp.strip().replace('\\n', '')\n",
        "\n",
        "                # If index page, we get too many full stops. Skip such pages\n",
        "                if text.count('.') / len(text) > .25:\n",
        "                    continue\n",
        "\n",
        "                # Convert the tokens to ids using encode function of tokenizer\n",
        "                input_ids = tokenizer.encode(question, text)\n",
        "                tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "                temp = np.array(tokens)\n",
        "                if len([x for x in temp if '#' in x]) / len(temp) > .5:\n",
        "                    continue\n",
        "\n",
        "                # Split the tokens\n",
        "                if len(tokens) > 450:\n",
        "                    factor, len_ = find_factors(len(tokens))\n",
        "                    temp = list(tokens[:len_ - 1])\n",
        "                    temp.append(tokens[-1])\n",
        "                    tokens = temp\n",
        "                    del temp\n",
        "                    temp = list(input_ids[:len_ - 1])\n",
        "                    temp.append(input_ids[-1])\n",
        "                    input_ids = temp\n",
        "                    del temp\n",
        "                    input_ids = np.array(input_ids).reshape((-1, factor))\n",
        "                    tokens = np.array(tokens).reshape((-1, factor))\n",
        "                else:\n",
        "                    tokens = [tokens]\n",
        "                    input_ids = [input_ids]\n",
        "\n",
        "                sep_idx = 0\n",
        "                for i in range(len(input_ids)):\n",
        "                    temp_tok = list(tokens[i])\n",
        "                    temp_id = list(input_ids[i])\n",
        "\n",
        "                    if i > 0:\n",
        "                        temp_id = list(input_ids[0][:sep_idx + 1])\n",
        "                        temp_id.extend(list(input_ids[i]))\n",
        "                        temp_tok = list(tokens[0][:sep_idx + 1])\n",
        "                        temp_tok.extend(list(tokens[i]))\n",
        "\n",
        "                    if tokens[i][-1] != '[SEP]':\n",
        "                        temp_id.append(input_ids[0][sep_idx])\n",
        "                        temp_tok.append(tokens[0][sep_idx])\n",
        "\n",
        "                    sep_idx = temp_id.index(tokenizer.sep_token_id)\n",
        "                    num_seg_a = sep_idx + 1\n",
        "                    num_seg_b = len(temp_id) - num_seg_a\n",
        "                    segment_ids = [0] * num_seg_a + [1] * num_seg_b\n",
        "                    assert len(segment_ids) == len(temp_id)\n",
        "\n",
        "                    output = model(torch.tensor([temp_id]), token_type_ids=torch.tensor([segment_ids]))\n",
        "\n",
        "                    answer_start = torch.argmax(output.start_logits)\n",
        "                    answer_end = torch.argmax(output.end_logits)\n",
        "\n",
        "                    if answer_end >= answer_start:\n",
        "                        ans = \" \".join(temp_tok[answer_start:answer_end + 1])\n",
        "                        if '[' not in ans.capitalize().strip() and len(ans.capitalize().strip()) != 0:\n",
        "                            answer += ans.capitalize().replace(' ##', '')\n",
        "\n",
        "    if len(answer) == 0:\n",
        "        answer = \"I am unable to find the answer to this question\"\n",
        "\n",
        "    return answer\n",
        "\n",
        "if __name__=='__main__':\n",
        "    question = \"What was M/s. Prakash Atlanta's case?\"\n",
        "    pdf_path = '/content/Union of India vs Ved Prakash Mithal and Sons- DELHI HC JUDGEMENT.pdf'\n",
        "    answer = findAnswerPDF(question, pdf_path)\n",
        "    print(f'Question : {question}')\n",
        "    print(f' Answer  : {answer}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XzynYVmJglMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}